# -*- coding: utf-8 -*-
"""AI Assignment - Task 1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v9O8sWGYj4iuqvWHTH1K198FQUj0h33o
"""

!pip install -q textblob nltk

import pandas as pd
import numpy as np
import re
import string
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from textblob import TextBlob
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import scipy

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt_tab')

from google.colab import files
uploaded = files.upload()

df = pd.read_excel("dataset.xls")
df.dropna(subset=['ticket_text', 'issue_type', 'urgency_level'], inplace=True)

def clean_text(text):
    text = text.lower()
    text = re.sub(r'\d+', '', text)
    text = re.sub(rf"[{string.punctuation}]", "", text)
    tokens = nltk.word_tokenize(text)
    tokens = [t for t in tokens if t not in stopwords.words('english')]
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(t) for t in tokens]
    return " ".join(tokens)

df['clean_text'] = df['ticket_text'].apply(clean_text)
df.head()

vectorizer = TfidfVectorizer(max_features=1000)
X_tfidf = vectorizer.fit_transform(df['clean_text'])

df['ticket_length'] = df['clean_text'].apply(lambda x: len(x.split()))
df['sentiment'] = df['ticket_text'].apply(lambda x: TextBlob(x).sentiment.polarity)

X_custom = df[['ticket_length', 'sentiment']].values
X_combined = scipy.sparse.hstack([X_tfidf, X_custom])

#issue type
X_train, X_test, y_train, y_test = train_test_split(X_combined, df['issue_type'], test_size=0.2, random_state=42)
model_issue = RandomForestClassifier()
model_issue.fit(X_train, y_train)
print("Issue Type Classification Report:")
print(classification_report(y_test, model_issue.predict(X_test)))

#urgency level
y_urg_train, y_urg_test = train_test_split(df['urgency_level'], test_size=0.2, random_state=42)
model_urgency = RandomForestClassifier()
model_urgency.fit(X_train, y_urg_train)
print("Urgency Level Classification Report:")
print(classification_report(y_urg_test, model_urgency.predict(X_test)))

!pip install dateparser
import dateparser.search

keywords = ['broken', 'delay', 'error', 'late', 'damaged', 'not working']

def extract_entities(text, product_list):
    text_lower = text.lower()
    found_products = [p for p in product_list if str(p).lower() in text_lower]
    dates = dateparser.search.search_dates(text)
    found_keywords = [k for k in keywords if k in text_lower]

    return {
        'products': found_products,
        'dates': [d[0] for d in dates] if dates else [],
        'keywords': found_keywords
    }

def process_ticket(text):
    cleaned = clean_text(text)
    tfidf = vectorizer.transform([cleaned])
    length = len(cleaned.split())
    sentiment = TextBlob(text).sentiment.polarity
    features = scipy.sparse.hstack([tfidf, [[length, sentiment]]])

    issue_pred = model_issue.predict(features)[0]
    urg_pred = model_urgency.predict(features)[0]
    entities = extract_entities(text, df['product'].dropna().unique())

    return {
        'issue_type': issue_pred,
        'urgency_level': urg_pred,
        'entities': entities
    }

#sample usecase
process_ticket("My laptop is broken and the screen was cracked on 2023-04-18")

import matplotlib.pyplot as plt
import seaborn as sns

# issue type distribution
plt.figure(figsize=(6,4))
sns.countplot(data=df, x='issue_type')
plt.title("Ticket Distribution by Issue Type")
plt.xticks(rotation=45)
plt.show()

# urgency level distribution
plt.figure(figsize=(6,4))
sns.countplot(data=df, x='urgency_level')
plt.title("Ticket Distribution by Urgency Level")
plt.show()

importances = model_issue.feature_importances_[-2:]  # last 2 are ticket's length and sentiment
plt.bar(['ticket_length', 'sentiment'], importances)
plt.title("Feature Importance (Custom Features)")
plt.show()

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# for the issue type
y_pred_issue = model_issue.predict(X_test)
ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_issue),
                       display_labels=model_issue.classes_).plot()
plt.title("Confusion Matrix - Issue Type")
plt.show()

# for the urgency level
y_pred_urgency = model_urgency.predict(X_test)

ConfusionMatrixDisplay(
    confusion_matrix(y_urg_test, y_pred_urgency),
    display_labels=model_urgency.classes_
).plot()

plt.title("Confusion Matrix - Urgency Level")
plt.show()

!pip install gradio
import gradio as gr

def batch_predict(ticket_texts):
    results = []
    for text in ticket_texts.strip().split('\n'):
        if text.strip():
            output = process_ticket(text)
            results.append({ 'input': text, **output })
    return results

gr.Interface(fn=batch_predict, inputs=gr.Textbox(lines=5, placeholder="Paste multiple tickets..."),
             outputs="json", title="Batch Ticket Classifier").launch()

import joblib
import os

os.makedirs('models', exist_ok=True)

joblib.dump(model_issue, 'models/issue_classifier.pkl')
joblib.dump(model_urgency, 'models/urgency_classifier.pkl')
joblib.dump(vectorizer, 'models/tfidf_vectorizer.pkl')

from google.colab import files
files.download('models/issue_classifier.pkl')
files.download('models/urgency_classifier.pkl')
files.download('models/tfidf_vectorizer.pkl')

